[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shiny Econometrics Apps",
    "section": "",
    "text": "Use the links below to open the interactive apps."
  },
  {
    "objectID": "index.html#ordinary-least-squares-ols-slope-estimate",
    "href": "index.html#ordinary-least-squares-ols-slope-estimate",
    "title": "Shiny Econometrics Apps",
    "section": "Ordinary Least Squares (OLS) Slope Estimate",
    "text": "Ordinary Least Squares (OLS) Slope Estimate\nThis is the Ordinary Least Squares (OLS) slope estimate.\n\\[\n\\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^n (\\mathit{x}_i - \\bar{\\mathit{x}})(\\mathit{y}_i - \\bar{\\mathit{y}})}{\\sum_{i=1}^n (\\mathit{x}_i - \\bar{\\mathit{x}})^2}\n\\] ‚ö° Open App\nüîß Check R Code on GitHub\nWhen Is OLS Defined?\nWe can compute \\(\\widehat{\\beta}_1\\) as long as the sample variance of \\(\\mathit{x}\\) is not 0.\nIf all individuals have the same \\(\\mathit{x}\\) (e.g., same schooling), the slope cannot be identified.\nA covariance matrix \\(\\Sigma\\) must satisfy two things to be valid (‚Äúpositive‚Äêdefinite‚Äù):\n\nEach variance on the diagonal must be strictly positive.\n\nThe determinant must be positive, i.e. \\[\n\\det(\\Sigma)\n= \\mathrm{Var}(X)\\,\\mathrm{Var}(Y)\\;-\\;\\bigl(\\mathrm{Cov}(X,Y)\\bigr)^2\n&gt;0.\n\\]\n\nIn the app you can drive\n- \\(\\mathrm{Var}(X)=\\mathit{var\\_x}\\),\n- \\(\\mathrm{Var}(Y)=\\mathit{var\\_y}\\),\n- \\(\\mathrm{Cov}(X,Y)=\\mathit{cov\\_xy}\\).\nIf you pick them so that \\[\n\\mathit{var\\_x}\\times \\mathit{var\\_y}\n\\;\\le\\;\\bigl(\\mathit{cov\\_xy}\\bigr)^2,\n\\] then \\(\\det(\\Sigma)\\le0\\) and the matrix is no longer positive‚Äêdefinite. In practice that means you‚Äôve asked R to draw from a ‚Äúcovariance matrix‚Äù that doesn‚Äôt correspond to any real bivariate Normal distribution.\nIntercept\nOnce we know \\(\\widehat{\\beta}_1\\), we compute the intercept:\n\\[\n\\widehat{\\beta}_0 = \\bar{\\mathit{y}} - \\widehat{\\beta}_1 \\bar{\\mathit{x}}\n\\] Squared Residuals: We measure prediction errors by squaring the residuals:\n\\[\nSSR = \\sum_{i=1}^n \\widehat{\\varepsilon}_i^2 = \\sum_{i=1}^n (\\mathit{y}_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 \\mathit{x}_i)^2\n\\]\nThis is the Sum of Squared Residuals (SSR). How do we pick \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) to minimize SSR?\nUsing calculus, we find that the minimizers are:\n\n\\(\\widehat{\\beta}_1\\): the slope formula from earlier\n\\(\\widehat{\\beta}_0 = \\bar{\\mathit{y}} - \\widehat{\\beta}_1 \\bar{\\mathit{x}}\\)\n\nSo minimizing SSR gives us the OLS estimators.\nThe OLS Regression Line:\nOnce we have estimated values:\n\\[\n\\widehat{\\mathit{y}}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\mathit{x}_i\n\\]\nThis is the OLS regression line ‚Äî our best linear prediction for \\(\\mathit{y}\\) given \\(\\mathit{x}\\).\nOnce we have the estimated coefficients and the OLS regression line, we can predict the outcome \\(\\mathit{y}\\) for any sensible value of \\(\\mathit{x}\\).\nJust plug in a value for \\(\\mathit{x}\\), and compute the predicted \\(\\mathit{y}\\):\n\\[\n\\widehat{\\mathit{y}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\mathit{x}\n\\]\nOLS provides the best linear prediction of \\(\\mathit{y}\\) in the sense that it minimizes the prediction error among all linear estimators.\nThere is always some prediction error, but OLS minimizes that error on average.\nIn fact, OLS is the Best Linear Unbiased Estimator (BLUE) ‚Äî it is the ‚Äúleast worst‚Äù linear prediction method for estimating \\(\\mathit{y}\\) from \\(\\mathit{x}\\)."
  },
  {
    "objectID": "index.html#consistency-vs-endogeneity",
    "href": "index.html#consistency-vs-endogeneity",
    "title": "Shiny Econometrics Apps",
    "section": "Consistency vs Endogeneity",
    "text": "Consistency vs Endogeneity\nWhat this app shows: As sample size (\\(n\\)) grows, OLS centers around the true slope if (\\(\\operatorname{Corr}(x,u)=0\\)). If you dial in endogeneity (\\((\\rho \\ne 0)\\)), the sampling distribution shifts: the estimator is biased/inconsistent.\nHow to use: Choose (n), the true (\\(\\beta_1\\)), and the correlation (\\(\\rho=\\operatorname{Corr}(x,u)\\)). Click Simulate to see the histogram of (\\(\\hat\\beta_1\\)) across Monte Carlo draws. The red line is the true (\\(\\beta_1\\)).\n‚ö° Open App\nüîß Check R Code on GitHub"
  },
  {
    "objectID": "index.html#interaction-and-nonlinearity",
    "href": "index.html#interaction-and-nonlinearity",
    "title": "Shiny Econometrics Apps",
    "section": "Interaction and Nonlinearity",
    "text": "Interaction and Nonlinearity\nWe now visualize interaction and nonlinearity. The marginal effect of (x) on (y) in the model \\[\ny=\\beta_0+\\beta_1 x+\\beta_2 z+\\beta_3 xz\n\\] is \\((\\partial y/\\partial x = \\beta_1 + \\beta_3 z)\\): it depends on (z). If you set ‚Äú(z=x)‚Äù you get a quadratic \\((y=\\beta_0 + (\\beta_1+\\beta_2)x + \\beta_3 x^2)\\) and \\((\\partial y/\\partial x = \\beta_1+\\beta_2+2\\beta_3 x)\\).\nThis Shiny app visualizes marginal effects and prediction curves for a linear model with an interaction (and an optional squared term), and it reports the regression output (Intercept, \\(x\\), \\(z\\), \\(x\\!:\\!z\\)). It also lets you inject noise to see how statistical significance shrinks (wider CIs, smaller \\(t\\)-stats).\n‚ö° Open App\nüîß Check R Code on GitHub\nChoose the Data-Generating Process (left panel)\n\n\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) ‚Äî set the ‚Äútrue‚Äù coefficients used to generate the data:\n\nModel (continuous/dummy): \\(y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 xz + u\\)\nModel (square): \\(z=x\\) and \\(y = \\beta_0 + (\\beta_1+\\beta_2) x + \\beta_3 x^2 + u\\)\n\nType of \\(z\\)\n\nContinuous: \\(z \\in [\\text{Range}]\\) (you choose the range)\n\nDummy: \\(z \\in \\{0,1\\}\\) (random 0/1)\n\n\\(z = x\\) (square): sets \\(z=x\\) and fits a quadratic in \\(x\\)\n\nRange of \\(z\\) (continuous only): pick the min/max values used in the plots.\n\nAdd Noise to Compress Significance\n\nOutcome noise SD (\\(\\sigma_u\\)) ‚Äî increases residual variance; bigger \\(\\sigma_u\\) \\(\\Rightarrow\\) wider CIs and smaller \\(t\\)-stats.\n\nHeteroskedastic noise ‚Äî if checked, noise scales with \\(1+|z|\\), making uncertainty larger when \\(|z|\\) is big.\n\nMeasurement error in \\(x\\) ‚Äî observed \\(x = x_{\\text{true}} + \\text{noise}\\); induces attenuation (slopes shrink toward 0) and larger SEs.\n\nTip: combine higher \\(\\sigma_u\\) and \\(x\\)-measurement error for a clear loss of significance.\n\n\nSimulation Controls\n\nSample size \\(n\\) ‚Äî affects precision; larger \\(n\\) tightens CIs (holding noise fixed).\n\nRandom seed ‚Äî set for reproducibility.\n\nResimulate sample ‚Äî click to draw a fresh sample under your current settings.\n\nOutputs (right panel)\n\nTop row: Two marginal-effect plots\n\nMarginal Effect of \\(x\\) on \\(y\\) (vs \\(x\\))\n\nTheoretical curves from your true \\(\\beta\\)‚Äôs:\n\nContinuous/Dummy \\(z\\): \\(\\frac{\\partial y}{\\partial x} = \\beta_1 + \\beta_3 z\\) (horizontal lines by \\(z\\) level)\n\\(z = x\\) (square): \\(\\frac{\\partial y}{\\partial x} = \\beta_1 + \\beta_2 + 2\\beta_3 x\\) (curved)\n\n\nMarginal Effect of \\(x\\) on \\(y\\) (vs \\(z\\)) with 95% CI\n\nEstimated from the fitted model on observed data with the current sample.\n\nContinuous: ribbon shows 95% CI over \\(z\\).\n\nDummy: points with error bars at \\(z=0\\) and \\(z=1\\).\n\nSquare: treats the horizontal axis as \\(z=x\\) and shows \\(\\hat{\\beta}_x + 2\\hat{\\beta}_{x^2} z\\) with CI.\n\n\nBottom row: Predicted \\(y\\) vs \\(x\\)\n\nDraws the true DGP curves (using your \\(\\beta\\)‚Äôs) for several \\(z\\)-levels (or a single curve when \\(z=x\\)).\n\nRegression output table\n\nShows the estimated coefficients and standard errors from the model fit on observed variables:\n\nContinuous/Dummy \\(z\\): (Intercept), \\(x\\), \\(z\\), \\(x\\!:\\!z\\)\n\n\\(z = x\\) (square): (Intercept), \\(x\\), \\(x^2\\) (here the interaction is represented by curvature)\n\n\n\nQuick Demos (copy-ready)\n\nShow interaction: set \\(\\beta_1=1\\), \\(\\beta_2=0.5\\), \\(\\beta_3=0.8\\); use Continuous \\(z\\), range \\([-3,3]\\). The ME plots reveal parallel shifts when \\(\\beta_3=0\\) and diverging lines when \\(\\beta_3 \\ne 0\\).\nShrink significance: increase \\(\\sigma_u\\) to \\(3\\)‚Äì\\(4\\) and add measurement error in \\(x\\) (e.g., \\(1.0\\)). Watch CIs widen and \\(t\\)-stats drop in the table.\nHeteroskedasticity: check the box; CI in the ME-vs-\\(z\\) plot widens in the tails.\n\nInterpretation Tips\n\nThe ME vs \\(x\\) plot uses true \\(\\beta\\)‚Äôs for clean pedagogy (what the DGP implies).\n\nThe ME vs \\(z\\) plot uses the fitted model (what your sample and noise deliver), including 95% CIs that react to \\(n\\), \\(\\sigma_u\\), heteroskedasticity, and measurement error.\n\nThe Regression table is your go-to for statistical significance (coefficients, SEs, \\(t\\)-values, \\(p\\)-values). Noise controls will compress significance by design.\n\nKnown Limits - For \\(z = x\\) (square), the app fits \\(y \\sim x + x^2\\); there is no separate \\(z\\) or \\(x\\!:\\!z\\) line in the table because the nonlinearity encodes the interaction.\n- For Dummy \\(z\\), the app keeps \\(z\\) exactly \\(0/1\\) (no measurement error in \\(z\\)) to avoid breaking the factor structure."
  },
  {
    "objectID": "index.html#iv-bias-weak-instruments",
    "href": "index.html#iv-bias-weak-instruments",
    "title": "Shiny Econometrics Apps",
    "section": "IV Bias & Weak Instruments",
    "text": "IV Bias & Weak Instruments\nExplore endogeneity (œÅ), instrument strength (Œ≥‚ÇÅ), first stage, reduced form, and the weak-instrument F-test.\n‚ö° Open App\nüîß Check R Code on GitHub"
  }
]