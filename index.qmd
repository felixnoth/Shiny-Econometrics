---
title: "Shiny Econometrics Apps"
format:
  html:
    toc: true
    toc-location: left   # or "right"
    toc-title: "Contents"
---

# Welcome

Use the links below to open the interactive apps.

# Regression Review I

## Ordinary Least Squares (OLS) Slope Estimate 
This is the **Ordinary Least Squares (OLS) slope estimate**.

$$
\widehat{\beta}_1 = \frac{\sum_{i=1}^n (\mathit{x}_i - \bar{\mathit{x}})(\mathit{y}_i - \bar{\mathit{y}})}{\sum_{i=1}^n (\mathit{x}_i - \bar{\mathit{x}})^2}
$$
[‚ö° Open App](https://felixnoth.shinyapps.io/ols_slope/){target="_blank"}

[üîß Check R Code on GitHub](https://github.com/felixnoth/Shiny-Econometrics/tree/main/apps/ols_slope){target="_blank"} 

**When Is OLS Defined?** 

We can compute $\widehat{\beta}_1$ as long as the **sample variance of $\mathit{x}$ is not 0**.

If all individuals have the same $\mathit{x}$ (e.g., same schooling), the slope **cannot be identified**.

A covariance matrix $\Sigma$ must satisfy two things to be valid (‚Äúpositive‚Äêdefinite‚Äù):

1. Each variance on the diagonal must be strictly positive.  
2. The determinant must be positive, i.e.
   $$
   \det(\Sigma)
   = \mathrm{Var}(X)\,\mathrm{Var}(Y)\;-\;\bigl(\mathrm{Cov}(X,Y)\bigr)^2
   >0.
   $$

In the app you can drive  
- $\mathrm{Var}(X)=\mathit{var\_x}$,  
- $\mathrm{Var}(Y)=\mathit{var\_y}$,  
- $\mathrm{Cov}(X,Y)=\mathit{cov\_xy}$.

If you pick them so that
$$
\mathit{var\_x}\times \mathit{var\_y}
\;\le\;\bigl(\mathit{cov\_xy}\bigr)^2,
$$
then $\det(\Sigma)\le0$ and the matrix is no longer positive‚Äêdefinite. In practice that means you‚Äôve asked R to draw from a ‚Äúcovariance matrix‚Äù that doesn‚Äôt correspond to any real bivariate Normal distribution.

**Intercept**

Once we know $\widehat{\beta}_1$, we compute the **intercept**:

$$
\widehat{\beta}_0 = \bar{\mathit{y}} - \widehat{\beta}_1 \bar{\mathit{x}}
$$
**Squared Residuals:** We measure prediction errors by **squaring the residuals**:

$$
SSR = \sum_{i=1}^n \widehat{\varepsilon}_i^2 = \sum_{i=1}^n (\mathit{y}_i - \widehat{\beta}_0 - \widehat{\beta}_1 \mathit{x}_i)^2
$$

This is the **Sum of Squared Residuals (SSR)**. How do we pick $\widehat{\beta}_0$ and $\widehat{\beta}_1$ to **minimize SSR**?

Using calculus, we find that the **minimizers** are:

- $\widehat{\beta}_1$: the slope formula from earlier
- $\widehat{\beta}_0 = \bar{\mathit{y}} - \widehat{\beta}_1 \bar{\mathit{x}}$

So minimizing SSR gives us the **OLS estimators**.

**The OLS Regression Line:**

Once we have estimated values:

$$
\widehat{\mathit{y}}_i = \widehat{\beta}_0 + \widehat{\beta}_1 \mathit{x}_i
$$

This is the **OLS regression line** ‚Äî our best linear prediction for $\mathit{y}$ given $\mathit{x}$.

Once we have the **estimated coefficients** and the **OLS regression line**, we can predict the outcome $\mathit{y}$ for any sensible value of $\mathit{x}$.

Just **plug in a value** for $\mathit{x}$, and compute the predicted $\mathit{y}$:

$$
\widehat{\mathit{y}} = \widehat{\beta}_0 + \widehat{\beta}_1 \mathit{x}
$$

OLS provides the **best linear prediction** of $\mathit{y}$ in the sense that it **minimizes the prediction error** among all linear estimators.

There is always **some prediction error**, but OLS **minimizes** that error **on average**.

In fact, OLS is the **Best Linear Unbiased Estimator (BLUE)** ‚Äî it is the "least worst" linear prediction method for estimating $\mathit{y}$ from $\mathit{x}$.

## IV Bias & Weak Instruments

Explore endogeneity (œÅ), instrument strength (Œ≥‚ÇÅ), first stage, reduced form, and the weak-instrument F-test.

[‚ö° Open App](https://felixnoth.shinyapps.io/iv_bias_weak_instruments/){target="_blank"}

[üîß Check R Code on GitHub](https://github.com/felixnoth/Shiny-Econometrics/tree/main/apps/iv_bias_weak_instruments){target="_blank"} 