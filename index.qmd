---
title: "Shiny Econometrics Apps"
format:
  html:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

# Welcome

Use the links below to open the interactive apps.

# Regression Review I

## Ordinary Least Squares (OLS) Slope Estimate 

This is the **Ordinary Least Squares (OLS) slope estimate**.

$$
\widehat{\beta}_1 = \frac{\sum_{i=1}^n (\mathit{x}_i - \bar{\mathit{x}})(\mathit{y}_i - \bar{\mathit{y}})}{\sum_{i=1}^n (\mathit{x}_i - \bar{\mathit{x}})^2}
$$
[‚ö° Open App](https://felixnoth.shinyapps.io/ols_slope/){target="_blank"}

[üîß Check R Code on GitHub](https://github.com/felixnoth/Shiny-Econometrics/tree/main/apps/ols_slope){target="_blank"} 

**When Is OLS Defined?** 

We can compute $\widehat{\beta}_1$ as long as the **sample variance of $\mathit{x}$ is not 0**.

If all individuals have the same $\mathit{x}$ (e.g., same schooling), the slope **cannot be identified**.

A covariance matrix $\Sigma$ must satisfy two things to be valid (‚Äúpositive‚Äêdefinite‚Äù):

1. Each variance on the diagonal must be strictly positive.  
2. The determinant must be positive, i.e.
   $$
   \det(\Sigma)
   = \mathrm{Var}(X)\,\mathrm{Var}(Y)\;-\;\bigl(\mathrm{Cov}(X,Y)\bigr)^2
   >0.
   $$

In the app you can drive  
- $\mathrm{Var}(X)=\mathit{var\_x}$,  
- $\mathrm{Var}(Y)=\mathit{var\_y}$,  
- $\mathrm{Cov}(X,Y)=\mathit{cov\_xy}$.

If you pick them so that
$$
\mathit{var\_x}\times \mathit{var\_y}
\;\le\;\bigl(\mathit{cov\_xy}\bigr)^2,
$$
then $\det(\Sigma)\le0$ and the matrix is no longer positive‚Äêdefinite. In practice that means you‚Äôve asked R to draw from a ‚Äúcovariance matrix‚Äù that doesn‚Äôt correspond to any real bivariate Normal distribution.

**Intercept**

Once we know $\widehat{\beta}_1$, we compute the **intercept**:

$$
\widehat{\beta}_0 = \bar{\mathit{y}} - \widehat{\beta}_1 \bar{\mathit{x}}
$$
**Squared Residuals:** We measure prediction errors by **squaring the residuals**:

$$
SSR = \sum_{i=1}^n \widehat{\varepsilon}_i^2 = \sum_{i=1}^n (\mathit{y}_i - \widehat{\beta}_0 - \widehat{\beta}_1 \mathit{x}_i)^2
$$

This is the **Sum of Squared Residuals (SSR)**. How do we pick $\widehat{\beta}_0$ and $\widehat{\beta}_1$ to **minimize SSR**?

Using calculus, we find that the **minimizers** are:

- $\widehat{\beta}_1$: the slope formula from earlier
- $\widehat{\beta}_0 = \bar{\mathit{y}} - \widehat{\beta}_1 \bar{\mathit{x}}$

So minimizing SSR gives us the **OLS estimators**.

**The OLS Regression Line:**

Once we have estimated values:

$$
\widehat{\mathit{y}}_i = \widehat{\beta}_0 + \widehat{\beta}_1 \mathit{x}_i
$$

This is the **OLS regression line** ‚Äî our best linear prediction for $\mathit{y}$ given $\mathit{x}$.

Once we have the **estimated coefficients** and the **OLS regression line**, we can predict the outcome $\mathit{y}$ for any sensible value of $\mathit{x}$.

Just **plug in a value** for $\mathit{x}$, and compute the predicted $\mathit{y}$:

$$
\widehat{\mathit{y}} = \widehat{\beta}_0 + \widehat{\beta}_1 \mathit{x}
$$

OLS provides the **best linear prediction** of $\mathit{y}$ in the sense that it **minimizes the prediction error** among all linear estimators.

There is always **some prediction error**, but OLS **minimizes** that error **on average**.

In fact, OLS is the **Best Linear Unbiased Estimator (BLUE)** ‚Äî it is the "least worst" linear prediction method for estimating $\mathit{y}$ from $\mathit{x}$.

## Consistency vs Endogeneity 

What this app shows: As sample size ($n$) grows, OLS centers around the true slope if ($\operatorname{Corr}(x,u)=0$). If you dial in endogeneity ($(\rho \ne 0)$), the sampling distribution shifts: the estimator is biased/inconsistent.

How to use: Choose (n), the true ($\beta_1$), and the correlation ($\rho=\operatorname{Corr}(x,u)$). Click Simulate to see the histogram of ($\hat\beta_1$) across Monte Carlo draws. The red line is the true ($\beta_1$).

[‚ö° Open App](https://felixnoth.shinyapps.io/ols_consistency/){target="_blank"}

[üîß Check R Code on GitHub](https://github.com/felixnoth/Shiny-Econometrics/tree/main/apps/ols_consistency){target="_blank"} 

# Regression Review II

## Interaction and Nonlinearity

We now visualize interaction and nonlinearity. The marginal effect of (x) on (y) in the model
$$
y=\beta_0+\beta_1 x+\beta_2 z+\beta_3 xz
$$
is $(\partial y/\partial x = \beta_1 + \beta_3 z)$: it depends on (z). If you set ‚Äú(z=x)‚Äù you get a quadratic $(y=\beta_0 + (\beta_1+\beta_2)x + \beta_3 x^2)$ and $(\partial y/\partial x = \beta_1+\beta_2+2\beta_3 x)$.

This Shiny app visualizes **marginal effects** and **prediction curves** for a linear model with an interaction (and an optional squared term), and it reports the **regression output** (Intercept, $x$, $z$, $x\!:\!z$). It also lets you **inject noise** to see how statistical significance shrinks (wider CIs, smaller $t$-stats).

[‚ö° Open App](https://felixnoth.shinyapps.io/ols_interactions/){target="_blank"}

[üîß Check R Code on GitHub](https://github.com/felixnoth/Shiny-Econometrics/tree/main/apps/ols_interactions){target="_blank"} 

**Choose the Data-Generating Process (left panel)**

- **$\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$** ‚Äî set the ‚Äútrue‚Äù coefficients used to generate the data:
  - Model (continuous/dummy): $y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 xz + u$
  - Model (square): $z=x$ and $y = \beta_0 + (\beta_1+\beta_2) x + \beta_3 x^2 + u$
- **Type of $z$**  
  - *Continuous*: $z \in [\text{Range}]$ (you choose the range)  
  - *Dummy*: $z \in \{0,1\}$ (random 0/1)  
  - *$z = x$ (square)*: sets $z=x$ and fits a quadratic in $x$
- **Range of $z$** (continuous only): pick the min/max values used in the plots.

**Add Noise to Compress Significance**

- **Outcome noise SD ($\sigma_u$)** ‚Äî increases residual variance; **bigger $\sigma_u$ $\Rightarrow$ wider CIs** and smaller $t$-stats.  
- **Heteroskedastic noise** ‚Äî if checked, noise scales with $1+|z|$, making uncertainty larger when $|z|$ is big.  
- **Measurement error in $x$** ‚Äî observed $x = x_{\text{true}} + \text{noise}$; induces **attenuation** (slopes shrink toward 0) and larger SEs.  
  - *Tip:* combine higher $\sigma_u$ and $x$-measurement error for a clear loss of significance.

**Simulation Controls**

- **Sample size $n$** ‚Äî affects precision; larger $n$ tightens CIs (holding noise fixed).  
- **Random seed** ‚Äî set for reproducibility.  
- **Resimulate sample** ‚Äî click to draw a fresh sample under your current settings.

**Outputs (right panel)**

- **Top row:** Two marginal-effect plots  

  1. **Marginal Effect of $x$ on $y$ (vs $x$)**  
     - Theoretical curves from your *true* $\beta$‚Äôs:
       - *Continuous/Dummy $z$:* $\frac{\partial y}{\partial x} = \beta_1 + \beta_3 z$ (horizontal lines by $z$ level)
       - *$z = x$ (square):* $\frac{\partial y}{\partial x} = \beta_1 + \beta_2 + 2\beta_3 x$ (curved)
  2. **Marginal Effect of $x$ on $y$ (vs $z$) with 95\% CI**  
     - Estimated **from the fitted model on observed data** with the current sample.  
     - Continuous: ribbon shows 95\% CI over $z$.  
     - Dummy: points with error bars at $z=0$ and $z=1$.  
     - Square: treats the horizontal axis as $z=x$ and shows $\hat{\beta}_x + 2\hat{\beta}_{x^2} z$ with CI.

- **Bottom row:** **Predicted $y$ vs $x$**  
  - Draws the *true* DGP curves (using your $\beta$‚Äôs) for several $z$-levels (or a single curve when $z=x$).

- **Regression output table**  
  - Shows the **estimated coefficients and standard errors** from the model fit on **observed** variables:  
    - *Continuous/Dummy $z$:* `(Intercept)`, $x$, $z$, $x\!:\!z$  
    - *$z = x$ (square):* `(Intercept)`, $x$, $x^2$ (here the interaction is represented by curvature)

**Quick Demos (copy-ready)**

- **Show interaction:** set $\beta_1=1$, $\beta_2=0.5$, $\beta_3=0.8$; use *Continuous* $z$, range $[-3,3]$. The ME plots reveal parallel shifts when $\beta_3=0$ and diverging lines when $\beta_3 \ne 0$.
- **Shrink significance:** increase $\sigma_u$ to $3$‚Äì$4$ and add measurement error in $x$ (e.g., $1.0$). Watch CIs widen and $t$-stats drop in the table.
- **Heteroskedasticity:** check the box; CI in the ME-vs-$z$ plot widens in the tails.

**Interpretation Tips**

- The **ME vs $x$** plot uses **true $\beta$‚Äôs** for clean pedagogy (what the DGP implies).  
- The **ME vs $z$** plot uses the **fitted model** (what your sample and noise deliver), including **95\% CIs** that react to $n$, $\sigma_u$, heteroskedasticity, and measurement error.  
- The **Regression table** is your go-to for **statistical significance** (coefficients, SEs, $t$-values, $p$-values). Noise controls will **compress** significance by design.

**Known Limits**
- For *$z = x$ (square)*, the app fits $y \sim x + x^2$; there is no separate $z$ or $x\!:\!z$ line in the table because the nonlinearity encodes the interaction.  
- For *Dummy* $z$, the app keeps $z$ exactly $0/1$ (no measurement error in $z$) to avoid breaking the factor structure.

# Instrumental Variables

## IV Bias & Weak Instruments

Explore endogeneity (œÅ), instrument strength (Œ≥‚ÇÅ), first stage, reduced form, and the weak-instrument F-test.

[‚ö° Open App](https://felixnoth.shinyapps.io/iv_bias_weak_instruments/){target="_blank"}

[üîß Check R Code on GitHub](https://github.com/felixnoth/Shiny-Econometrics/tree/main/apps/iv_bias_weak_instruments){target="_blank"} 